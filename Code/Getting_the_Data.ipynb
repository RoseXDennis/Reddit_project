{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want 1,000 unique posts from each subreddit so we don't have imbalanced classes. My process was to take the maximum of each api pushshift (500) and sort by ascending and descending. Then, I dropped the duplicates, waited a day and gathered the rest of the data until I had 1000 observations from each subreddit. There was probably a better way of doing this by specifying what timeframe I'm getting the data from so I wouldn't have to wait a day to get new data. Note, at the end I've outputted my final data frame of both subreddits and will just use this in my exploratory analysis. If we run this again, the data will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our dataset, we only need the last cell of this notebook. When we explore the data we will just read in the dataset that we made in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shower Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good to have equal size of both datasets? yes, we want 1000 from each subreddit\n",
    "show_url1 = 'https://api.pushshift.io/reddit/search/submission?subreddit=showerthoughts&size=500'\n",
    "show_url2 = 'https://api.pushshift.io/reddit/search/submission?subreddit=showerthoughts&size=500&sort=asc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_req1 = requests.get(show_url1)\n",
    "show_req2 = requests.get(show_url2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_req1.status_code)\n",
    "\n",
    "print(show_req2.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower1 = show_req1.json()\n",
    "shower1 = shower1['data']\n",
    "shower1 = pd.DataFrame(shower1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower2 = show_req2.json()\n",
    "shower2 = shower2['data']\n",
    "shower2 = pd.DataFrame(shower2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower1 = shower1[['created_utc', 'title', 'selftext', 'subreddit', 'permalink', 'author']]\n",
    "shower2 = shower2[['created_utc', 'title', 'selftext', 'subreddit', 'permalink', 'author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([shower1, shower2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp['title'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.drop_duplicates(subset = 'title', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp['title']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave us 995 unique observations. We need 5 more but have to wait some time to run the api again because otherwise, it'll just take the same observations. Our final shower dataframe will be a combination of this temporary one and 5 new observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('temp.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stoner philosophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stone_url1 = 'https://api.pushshift.io/reddit/search/submission?subreddit=stonerphilosophy&size=500'\n",
    "stone_url2 = 'https://api.pushshift.io/reddit/search/submission?subreddit=stonerphilosophy&size=500&sort=asc'\n",
    "\n",
    "stone_req1 = requests.get(stone_url1)\n",
    "stone_req2 = requests.get(stone_url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stone_req1.status_code)\n",
    "print(stone_req2.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoner1 = stone_req1.json()\n",
    "stoner1 = stoner1['data']\n",
    "stoner1 = pd.DataFrame(stoner1)\n",
    "\n",
    "stoner2 = stone_req2.json()\n",
    "stoner2 = stoner2['data']\n",
    "stoner2 = pd.DataFrame(stoner2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoner1 = stoner1[['created_utc', 'title', 'selftext', 'subreddit', 'permalink', 'author']]\n",
    "stoner2 = stoner2[['created_utc', 'title', 'selftext', 'subreddit', 'permalink', 'author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = pd.concat([stoner1, stoner2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp2['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.drop_duplicates(subset = 'title', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp2['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough this also gave us 995 unique observations. We need 5 more but have to wait some time to run the api again because otherwise, it'll just take the same observations. Our final stoner dataframe will be a combination of this temporary one and 5 new observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.to_csv('temp2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting for 5 more unique observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting 'extra' showerthoughts observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('../Data/temp.csv')\n",
    "url1 = 'https://api.pushshift.io/reddit/search/submission?subreddit=showerthoughts&size=10'\n",
    "req1 = requests.get(url1)\n",
    "temp1 = req1.json()\n",
    "temp1 = temp1['data']\n",
    "temp1 = pd.DataFrame(temp1)\n",
    "temp1 = temp1[['created_utc', 'title', 'selftext', 'subreddit', 'permalink', 'author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([temp, temp1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['title'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dataframe had 995 values and it seems like we've added 10 whole new observations. So we will save this dataframe to a new csv, after deleting 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the last 5 so we have 1000 observations\n",
    "df = df[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('shower_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting 'extra' stonerphilosophy observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There wasn't enough new posts so we have to get new data a different way. We will use the 'before' parameter in the api pushshift link. I located the 'created_utc' of the first observation and said I want posts after a random number on that magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('../Data/temp2.csv')\n",
    "url1 = 'https://api.pushshift.io/reddit/search/submission?subreddit=stonerphilosophy&size=20&after=1480139035'\n",
    "req1 = requests.get(url1)\n",
    "temp1 = req1.json()\n",
    "temp1 = temp1['data']\n",
    "temp1 = pd.DataFrame(temp1)\n",
    "temp1 = temp1[['created_utc', 'title', 'selftext', 'subreddit', 'permalink', 'author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([temp, temp1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[:-15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('stoner_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine both final shower and stoner dataframes to create our final data frame that we will work with for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/shower_final.csv')\n",
    "df2 = pd.read_csv('../Data/stoner_final.csv')\n",
    "\n",
    "final_df = pd.concat([df, df2], axis=0)\n",
    "final_df.to_csv('../Data/final.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
